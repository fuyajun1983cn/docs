--- 
layout: default
---
<div class="header">
  <div class="jumbotron">
    <div class="container">
      <div class="row">
        <div class="col-sm-12">
          <h1>Overview of Linux RX Path</h1>
          <p>本文概述了Linux系统中从网卡接收到数据到最终到达应用层所经历的主要过程</p>
        </div>
      </div>
    </div>
  </div>
</div>

<div id="content" class="container">
<div class="row"><div class="col-md-9">


<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> 引用原文地址</h2>
<div class="outline-text-2" id="text-1">
<p>
<a href="https://blog.packagecloud.io/eng/2016/10/11/monitoring-tuning-linux-networking-stack-receiving-data-illustrated/">https://blog.packagecloud.io/eng/2016/10/11/monitoring-tuning-linux-networking-stack-receiving-data-illustrated/</a>
</p>
</div>
</div>
<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2"><span class="section-number-2">2</span> Getting started</h2>
<div class="outline-text-2" id="text-2">
<p>
These diagrams are intended to give an overview of how the Linux
network stack works.  The purpose of these drawings is to help
readers form a mental model of how some of the systems in the kernel
interact with each other at a high-level. 
</p>

<p>
Let’s begin by taking a look at some important initial setup that
is necessary before packet processing can be understood.
</p>
</div>
</div>
<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3"><span class="section-number-2">3</span> Initial setup</h2>
<div class="outline-text-2" id="text-3">

<figure>
<p><img src="./images/softirq.jpg" class="img-responsive" alt="softirq.jpg">
</p>
</figure>

<p>
Devices have many ways of alerting the rest of the computer system
that some work is ready for processing.   In the case of network
devices, it is common for the NIC to <b>raise an IRQ to signal</b> that a
packet has arrived and is ready to be processed.   When an IRQ
handler is executed by the Linux kernel, it runs at a very, very
high priority and often blocks additional IRQs from being
generated. As such, IRQ handlers in device drivers must execute as
quickly as possible and defer all long running work to execute
outside of this context. This is why the “softIRQ” system exists. 
</p>

<p>
Notes: 驱动中的中断处理函数必须尽快的执行完，避免Block整个系统对其他
中断的处理。 其他比较耗时的任务交给“软中断”去处理（下半部机制）。
</p>

<p>
The “softIRQ” system in the Linux kernel is a system that kernel
uses to process work outside of the device driver IRQ context. In
the case of network devices, the softIRQ system is responsible for
processing incoming packets. The softIRQ system is initialized early
during the boot process of the kernel.
</p>

<p>
Notes： 处理接收数据的主要逻辑是在软中断中执行的。
</p>

<p>
The diagram shows the initializing of the softIRQ system and its
per-CPU kernel threads. 
</p>

<p>
The initialization of the softIRQ system is as follows:
</p>
<ol class="org-ol">
<li>softIRQ kernel threads are created (one per CPU) in
<code>spawn_ksoftirqd</code> in kernel/softirq.c with a call to
<code>smpboot_register_percpu_thread</code> from kernel/smpboot.c. As seen in
the code, the function <code>run_ksoftirqd</code> is listed as <code>thread_fn</code>,
which is the function that will be executed in a loop.
</li>
<li>The ksoftirqd threads begin executing their processing loops in
the <code>run_ksoftirqd</code> function.
</li>
<li>Next, the <code>softnet_data</code> structures are created, one per CPU. These
structures hold  references to important data structures for
processing network data. One we’ll see again is  the
<code>poll_list</code>. The <code>poll_list</code> is where NAPI poll worker structures
will be added by calls  to <code>napi_schedule</code> or other NAPI APIs from
device drivers.
</li>
<li><code>net_dev_init</code> then registers the <code>NET_RX_SOFTIRQ</code> softirq with the
softirq system by calling <code>open_softirq</code>, as shown here. The
handler function that is registered is called <code>net_rx_action</code>. This
is the function the softirq kernel threads will execute to
process packets.
</li>
</ol>

<p class="note info">
  <h5>提示</h5>
  <p>
NOTES: 当驱动收到中断通知新接收的数据到来时，它会立即执行必要的动作，
然后调用 <code>napi_schedule</code> 交给Linux内核处理。 <code>net_rx_action</code> 是内核
处理接收到的数据的入口。
  </p>
</p>

<p>
Steps 5 - 8 on the diagram relate to the arrival of data for
processing and will be mentioned in the next section. Read on for
more!
</p>
</div>
</div>
<div id="outline-container-sec-4" class="outline-2">
<h2 id="sec-4"><span class="section-number-2">4</span> Data arrives</h2>
<div class="outline-text-2" id="text-4">

<figure>
<p><img src="./images/nic_dma.jpg" class="img-responsive" alt="nic_dma.jpg">
</p>
</figure>

<p>
Data arrives from the network!
</p>

<p>
When network data arrives at a NIC, the NIC will use <code>DMA</code> to write
the packet data to RAM. In the case of the <code>igb</code> network driver, a
ring buffer is setup in RAM that points to received packets. It is
important to note that some NICs are “multiqueue” NICs, meaning
that they can DMA incoming packets to one of many ring buffers in
RAM. As we’ll see soon, such NICs are able to make use of multiple
processors for processing incoming network data. The diagram above
shows just a single ring buffer for simplicity, but depending on the
NIC you are using and your hardware settings you may have multiple
queues on your system. 
</p>

<p>
Let’s walk through the process of receiving data:
</p>
<ol class="org-ol">
<li>Data is received by the NIC from the network.
</li>
<li>The NIC uses DMA to write the network data to RAM.
</li>
<li>The NIC raises an IRQ.
</li>
<li>The device driver’s registered IRQ handler is executed.
</li>
<li>The IRQ is cleared on the NIC, so that it can generate IRQs for new packet arrivals.
</li>
<li>NAPI softIRQ poll loop is started with a call to <code>napi_schedule</code>.
</li>
</ol>


<p>
The call to <code>napi_schedule</code> triggers the start of steps 5 - 8 in the
previous diagram. As we’ll see, the NAPI softIRQ poll loop is started
by simply flipping a bit in a bitfield and adding a structure to the
<code>poll_list</code> for processing. No other work is done by <code>napi_schedule</code> and
this is precisely how a driver defers processing to the softIRQ
system. 
</p>

<p>
Continuing on to the diagram in the previous section, using the
numbers found there:
</p>
<ol class="org-ol">
<li>(5)The call to <code>napi_schedule</code> in the driver adds the driver's NAPI
poll structure to the <code>poll_list</code> for the current CPU.
</li>
<li>(6)The softirq pending bit is set so that the <code>ksoftirqd</code> process on
this CPU knows that there are packets to process.
</li>
<li>(7) <code>run_ksoftirqd</code> function (which is being run in a loop by the
ksoftirq kernel thread) executes.
</li>
<li>(8) <code>__do_softirq</code> is called which checks the pending bitfield, sees
that a softIRQ is pending, and calls the handler registered for
the pending softIRQ: <code>net_rx_action</code> which does all the heavy
lifting for incoming network data processing. 
</li>
</ol>

<p>
It is important to note that the softIRQ kernel thread is executing
<code>net_rx_action</code>, not the device driver IRQ handler.
</p>
</div>
</div>
<div id="outline-container-sec-5" class="outline-2">
<h2 id="sec-5"><span class="section-number-2">5</span> Network data processing begins</h2>
<div class="outline-text-2" id="text-5">

<figure>
<p><img src="./images/net_rx_action.jpg" class="img-responsive" alt="net_rx_action.jpg">
</p>
</figure>

<p>
Now, data processing begins. The <code>net_rx_action</code> function (called from
the ksoftirqd kernel thread) will start to process any NAPI poll
structures that have been added to the <code>poll_list</code> for the current
CPU. Poll structures are added in two general cases: 
</p>
<ol class="org-ol">
<li>From device drivers with calls to <code>napi_schedule</code>.
</li>
<li>With an Inter-processor Interrupt in the case of Receive Packet
Steering. 
</li>
</ol>

<p>
We’re going to start by walking through what happens when a
driver’s NAPI structure is retreived from the <code>poll_list</code>.  
</p>

<p>
The diagram above can be summarized as follows:
</p>
<ol class="org-ol">
<li><code>net_rx_action</code> loop starts by checking the NAPI poll list for NAPI
structures.
</li>
<li>The <code>budget</code> and elapsed time are checked to ensure that the
softIRQ will not monopolize CPU time.
</li>
<li>The registered poll function is called. In this case, the
function <code>igb_poll</code> was registered by the igb driver.
</li>
<li>The driver’s <code>poll</code> function harvests packets from the ring buffer
in RAM.
</li>
<li>Packets are handed over to <code>napi_gro_receive</code>, which will deal with
possible Generic Receive Offloading.

  <p class="note info">
    <h5>GRO是什么？</h5>
    <p>
Generic Receive Offloading (GRO) is a software
implementation of a hardware optimization that is known as Large
Receive Offloading (LRO), (把小数据包组成大的数据再往协议栈层去送)
</p>
</p>
</li>
<li>Packets are either held for GRO and the call chain ends here or
packets are passed on to <code>net_receive_skb</code> to proceed up toward the
protocol stacks. 
</li>
</ol>


<p>
We’ll see next how net<sub>receive</sub><sub>skb</sub> deals with Receive Packet
steering to distribute the packet processing load amongst multiple
CPUs.
</p>
</div>
</div>
<div id="outline-container-sec-6" class="outline-2">
<h2 id="sec-6"><span class="section-number-2">6</span> Network data processing continues</h2>
<div class="outline-text-2" id="text-6">

<figure>
<p><img src="./images/netif_receive_skb.jpg" class="img-responsive" alt="netif_receive_skb.jpg">
</p>
</figure>

<p>
Network data processing continues from <code>netif_receive_skb</code>, but the
path of the data depends on whether or not Receive Packet Steering
(RPS) is enabled or not. An “out of the box” Linux kernel will not
have RPS enabled by default and it will need to be explicitly
enabled and configured if you want to use it. 
</p>

<p>
In the case where RPS is disabled, using the numbers in the above
diagram: 
</p>
<ol class="org-ol">
<li>(1) <code>netif_receive_skb</code> passes the data on to <code>__netif_receive_core</code>.
</li>
<li>(6) <code>__netif_receive_core</code> delivers data to any taps (like PCAP).
</li>
<li>(7) <code>__netif_receive_core</code> delivers data to registered protocol layer
handlers. In many cases, this would be the <code>ip_rcv</code> function that
the IPv4 protocol stack has registered. 

<p>
In the case where RPS is enabled:
</p>
<ol class="org-ol">
<li><code>netif_receive_skb</code> passes the data on to <code>enqueue_to_backlog</code>.
</li>
<li>Packets are placed on a per-CPU input queue for processing.
</li>
<li>The remote CPU’s NAPI structure is added to that CPU’s
<code>poll_list</code> and an IPI is queued which will trigger the softIRQ
kernel thread on the remote CPU to wake-up if it is not
running already.
</li>
<li>When the ksoftirqd kernel thread on the remote CPU runs, it
follows the same pattern describe in the previous section, but
this time, the registered poll function is <code>process_backlog</code>
which harvests packets from the current CPU’s input queue.
</li>
<li>Packets are passed on toward <code>__net_receive_skb_core</code>.
</li>
<li><code>__netif_receive_core</code> delivers data to any taps (like PCAP).
</li>
<li><code>__netif_receive_core</code> delivers data to registered protocol
layer handlers. In many cases, this would be the <code>ip_rcv</code>
function that the IPv4 protocol stack has registered.
</li>
</ol>
</li>
</ol>
</div>
</div>
<div id="outline-container-sec-7" class="outline-2">
<h2 id="sec-7"><span class="section-number-2">7</span> Protocol stacks and userland sockets</h2>
<div class="outline-text-2" id="text-7">
<p>
Next up are the protocol stacks, netfilter, berkley packet filters,
and finally the userland socket. This  code path is long, but linear
and relatively straightforward. 
</p>

<p>
You can continue following the detailed path for network data. A
very brief, high level summary of the path is: 
</p>
<ol class="org-ol">
<li>Packets are received by the IPv4 protocol layer with <code>ip_rcv</code>.
</li>
<li>Netfilter and a routing optimization are performed.
</li>
<li>Data destined for the current system is delivered to higher-level protocol layers, like UDP.
</li>
<li>Packets are received by the UDP protocol layer with <code>udp_rcv</code> and
are queued to the receive buffer of a userland socket by
<code>udp_queue_rcv_skb</code> and <code>sock_queue_rcv</code>. Prior to queuing to the
receive buffer, berkeley packet filters are processed. 
</li>
</ol>


<p>
Note that netfilter is consulted multiple times throughout this
process.
</p>
</div>
</div>
<div id="outline-container-sec-8" class="outline-2">
<h2 id="sec-8"><span class="section-number-2">8</span> Conclusion</h2>
<div class="outline-text-2" id="text-8">
<p>
The Linux network stack is incredibly complex and has many different
systems interacting together. Any effort to tune or monitor these
complex systems must strive to understand the interation between all
of them and how changing settings in one system will affect others. 
</p>
</div>
</div>
</div><div class="col-md-3"><nav id="table-of-contents">
<div id="text-table-of-contents" class="bs-docs-sidebar">
<ul class="nav">
<li><a href="#sec-1">1. 引用原文地址</a></li>
<li><a href="#sec-2">2. Getting started</a></li>
<li><a href="#sec-3">3. Initial setup</a></li>
<li><a href="#sec-4">4. Data arrives</a></li>
<li><a href="#sec-5">5. Network data processing begins</a></li>
<li><a href="#sec-6">6. Network data processing continues</a></li>
<li><a href="#sec-7">7. Protocol stacks and userland sockets</a></li>
<li><a href="#sec-8">8. Conclusion</a></li>
</ul>
</div>
</nav>
</div></div></div>
